この記事は SLP KBIT Advent Calendar 2020 12日目の記事です。~~（じゃあなぜ 13日に公開しているのか）~~
[https://adventar.org/calendars/5402:embed:cite]

# はじめに

機械学習を勉強する上で、重要なことはいくつもあると思います。  
その中でも本記事は全体像の部分、機械学習のおおまかな流れに焦点を当てます。  

機械学習を勉強し続けてはや2年。機械学習の分野は学ぶことが多く、勉強したてのときは気にも止めていなかったおおまかな流れですが、これを理解しているのとしていないとのでは今後の理解度に大きく響くと感じています。  

ですので、本記事では〇〇モデルの具体的なアルゴリズムや数学的内容(理論)だったり、精度向上の前処理だったりは省いてますのでご了承を。  

機械学習勉強したての後輩の誰かの目に入れば幸いです。

# 機械学習のおおまかな流れ

下図が機械学習のおおまかな流れです。大きく二つにわかれます。  

一つ目は<b>学習</b>です。すでに手元にあるデータを学習データとして入力し、モデルを出力します。  
モデルとはデータからある目的を解くために機械が分かる形に数値化(数式化)したものです。書いてて思いますが、非常にわかりにくい。  
ある目的には大きく分類と回帰があります。分類は例えば犬と猫の画像を分類する。回帰は株価を予測するなどがあります。  

二つ目は<b>推論</b>です。モデルと新しく入手したデータを現在データとしてマッチングさせることで、結果を出力します。  
推論と書いてますが、これは分類や回帰など目的によって変わるため、一般的に推論と呼ばれています。そのため出力結果は二値分類なら0or1、回帰なら予測値になります。  

長くなりましたが、この<b>学習</b>と<b>推論</b>がごっちゃにならないようにしようぜ、というのが言いたいことです。

[f:id:Noleff:20201213014755p:plain]

# 線形回帰

機械学習を学ぶとき線形回帰から触れることが多いんじゃないでしょうか。数学的な理論を理解するには大学レベルの数学はいりますが、直感的なイメージは中学生でも理解できるからです。  
Excelの散布図で近似曲線を引いたことがある人はすぐに理解できると思います。  そうです、y=ax+bです。

例えば人の身長を予測したいとします。身長が高ければ、体重も重くなるでしょう。これは正の相関があると言えます。正の相関があるということは体重からある程度身長がいくつか予測できることはなんとなくわかるかと思います。  

ここで、身長（予測対象の変数）を一般的に目的変数と呼びます。対して体重（予測するために使う変数）を説明変数と言います。前者は一つ、後者は一つ以上と考えてください。    
説明変数は今は体重だけです。人には痩せ型の人もいれば、太っている人もいるでしょう。そのため体重からだけで身長を正確に予測することは不可能に近いです。  

では別に年齢、性別、手足の大きさ、腕の長さ、足の長さなどなど、人に関する情報がより詳しくデータとしてあるとしましょう。体重だけよりも正確に予測できることは想像できると思います。  

線形回帰の話はこれくらいにしますが、より詳しく知りたい人は参考文献を参照してみてください。

# データ

[ボストン市郊外の地域別住宅価格](http://archive.ics.uci.edu/ml/index.php)のデータを使わさせていただきます。線形回帰を使う機械学習のデータセットとして一番有名だと思います。

# プログラム

 Pythonは機械学習関連のライブラリが豊富ですが、ライブラリがあまりにできすぎていて、ほとんど数行足らずで書き終わります。そのため実装は簡単でも本質的に理解しにくく、非常に混乱してしまいます。  

その結果、世の中にある機械学習のコードはほとんど、学習と推論を一括で書かれているケースが多いです。数行で書けるゆえの弊害ですかね。  

とは言ったものの、世の中の実際のデータは機械学習のデータセットのようにきれい整形されていないため、地獄の前処理に苦しむわけですが、ここでは割愛。

学習と推論を強調するためにモデルをファイルとして一度保存してます。   
ライブラリを使い倒してますが、とりあえずそこは一旦横に置いておいて。  
図と照らし合わせながら読むと良いかと思います。おおまかな流れだけを意識してみてください。 

- X_train: 学習データ（説明変数）
- y_train: 学習データ（目的変数）
- X_test:  現在データ（説明変数）
- y_test:  現在データ（目的変数）

[https://gist.github.com/b11b42b7ac823d54430f5cacf418e4d2:embed#gistb11b42b7ac823d54430f5cacf418e4d2]

[gistb11b42b7ac823d54430f5cacf418e4d2](https://gist.github.com/b11b42b7ac823d54430f5cacf418e4d2)

#  まとめ

機械学習のおおまかな流れについてまとめました。  
繰り返しになりますが学習と推論です。可視化などしてますが、説明変数を決めるためにしているだけです。  
線形回帰の精度評価は本記事では行いません。当たり前ですが、何の前処理もしていないので、精度は低いことは明らかです。  
[ボストン市郊外の地域別住宅価格](http://archive.ics.uci.edu/ml/index.php)を扱った記事は多くあると思いますので、そちらをご参照を。  
キーワードとしては標準化、重回帰あたりを調べると精度が向上するかと思います。

# 参考文献

[http://archive.ics.uci.edu/ml/index.php]
[https://qiita.com/0NE_shoT_/items/08376b08783cd554b02e]
[https://momonoki2017.blogspot.com/2018/01/scikit-learn_28.html]
[https://localab.jp/blog/save-and-load-machine-learning-models-in-python-with-scikit-learn/]

画像が見えにくいところもあるので、GitHubのリンクも載せておきます。

[https://github.com/furu8/blogress/blob/master/2020-10/ML.ipynb:embed:cite]


