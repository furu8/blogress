(764, 128, 128, 3)
(1052, 128, 128, 3)
(784, 128, 128, 3)
(733, 128, 128, 3)
(984, 128, 128, 3)
41
(7917, 128, 128, 3)
(7917,)
(5278, 128, 128, 3)
(5278, 41)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 124, 124, 16)      1216      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 41, 41, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 37, 37, 64)        25664     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 10, 10, 256)       147712    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 256)         0         
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 256)         0         
_________________________________________________________________
flatten (Flatten)            (None, 6400)              0         
_________________________________________________________________
dense (Dense)                (None, 2560)              16386560  
_________________________________________________________________
dropout_1 (Dropout)          (None, 2560)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 640)               1639040   
_________________________________________________________________
dense_2 (Dense)              (None, 128)               82048     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 41)                5289      
=================================================================
Total params: 18,287,529
Trainable params: 18,287,529
Non-trainable params: 0
_________________________________________________________________
(5278, 128, 128, 3)
(5278, 41)
(2639, 128, 128, 3)
(2639, 41)
Epoch 1/100
164/164 - 26s - loss: 3.2790 - accuracy: 0.1740 - val_loss: 2.7318 - val_accuracy: 0.2418
Epoch 2/100
164/164 - 17s - loss: 2.6838 - accuracy: 0.2610 - val_loss: 2.5012 - val_accuracy: 0.2581
Epoch 3/100
164/164 - 15s - loss: 2.5109 - accuracy: 0.2880 - val_loss: 2.3937 - val_accuracy: 0.3323
Epoch 4/100
164/164 - 15s - loss: 2.3513 - accuracy: 0.3218 - val_loss: 2.2071 - val_accuracy: 0.3444
Epoch 5/100
164/164 - 15s - loss: 2.2642 - accuracy: 0.3572 - val_loss: 2.2464 - val_accuracy: 0.3831
Epoch 6/100
164/164 - 15s - loss: 2.1696 - accuracy: 0.3788 - val_loss: 2.0370 - val_accuracy: 0.3899
Epoch 7/100
164/164 - 15s - loss: 2.1239 - accuracy: 0.3841 - val_loss: 1.9985 - val_accuracy: 0.4153
Epoch 8/100
164/164 - 15s - loss: 2.0629 - accuracy: 0.4127 - val_loss: 1.9855 - val_accuracy: 0.4218
Epoch 9/100
164/164 - 15s - loss: 2.0136 - accuracy: 0.4123 - val_loss: 2.0057 - val_accuracy: 0.4278
Epoch 10/100
164/164 - 15s - loss: 1.9737 - accuracy: 0.4272 - val_loss: 1.8941 - val_accuracy: 0.4407
Epoch 11/100
164/164 - 15s - loss: 1.9718 - accuracy: 0.4329 - val_loss: 1.8733 - val_accuracy: 0.4593
Epoch 12/100
164/164 - 15s - loss: 1.8659 - accuracy: 0.4459 - val_loss: 1.9365 - val_accuracy: 0.4346
Epoch 13/100
164/164 - 15s - loss: 1.8620 - accuracy: 0.4524 - val_loss: 1.8611 - val_accuracy: 0.4566
Epoch 14/100
164/164 - 15s - loss: 1.8211 - accuracy: 0.4674 - val_loss: 1.8013 - val_accuracy: 0.4752
Epoch 15/100
164/164 - 14s - loss: 1.7634 - accuracy: 0.4756 - val_loss: 1.8060 - val_accuracy: 0.4615
Epoch 16/100
164/164 - 15s - loss: 1.7513 - accuracy: 0.4834 - val_loss: 1.8260 - val_accuracy: 0.4695
Epoch 17/100
164/164 - 15s - loss: 1.7448 - accuracy: 0.4868 - val_loss: 1.7270 - val_accuracy: 0.4972
Epoch 18/100
164/164 - 15s - loss: 1.6774 - accuracy: 0.5008 - val_loss: 1.8273 - val_accuracy: 0.4786
Epoch 19/100
164/164 - 14s - loss: 1.6500 - accuracy: 0.5095 - val_loss: 1.8801 - val_accuracy: 0.4729
Epoch 20/100
164/164 - 14s - loss: 1.6297 - accuracy: 0.5105 - val_loss: 1.7999 - val_accuracy: 0.4956
Epoch 21/100
164/164 - 14s - loss: 1.5967 - accuracy: 0.5233 - val_loss: 1.7376 - val_accuracy: 0.5112
Epoch 22/100
164/164 - 14s - loss: 1.5441 - accuracy: 0.5252 - val_loss: 1.7555 - val_accuracy: 0.5002
Epoch 23/100
164/164 - 14s - loss: 1.5939 - accuracy: 0.5246 - val_loss: 1.8163 - val_accuracy: 0.4809
Epoch 24/100
164/164 - 14s - loss: 1.5814 - accuracy: 0.5313 - val_loss: 1.7775 - val_accuracy: 0.4968
Epoch 25/100
164/164 - 14s - loss: 1.5492 - accuracy: 0.5360 - val_loss: 1.6612 - val_accuracy: 0.5275
Epoch 26/100
164/164 - 14s - loss: 1.4897 - accuracy: 0.5559 - val_loss: 1.6365 - val_accuracy: 0.5252
Epoch 27/100
164/164 - 14s - loss: 1.5058 - accuracy: 0.5381 - val_loss: 1.7696 - val_accuracy: 0.4862
Epoch 28/100
164/164 - 14s - loss: 1.4725 - accuracy: 0.5579 - val_loss: 1.6723 - val_accuracy: 0.5108
Epoch 29/100
164/164 - 14s - loss: 1.4314 - accuracy: 0.5719 - val_loss: 1.6219 - val_accuracy: 0.5441
Epoch 30/100
164/164 - 14s - loss: 1.4380 - accuracy: 0.5644 - val_loss: 1.8224 - val_accuracy: 0.4915
Epoch 31/100
164/164 - 14s - loss: 1.4122 - accuracy: 0.5677 - val_loss: 1.6875 - val_accuracy: 0.5229
Epoch 32/100
164/164 - 14s - loss: 1.4024 - accuracy: 0.5772 - val_loss: 1.6301 - val_accuracy: 0.5225
Epoch 33/100
164/164 - 14s - loss: 1.3508 - accuracy: 0.5950 - val_loss: 1.7116 - val_accuracy: 0.5483
Epoch 34/100
164/164 - 14s - loss: 1.3310 - accuracy: 0.5909 - val_loss: 1.6494 - val_accuracy: 0.5343
Epoch 35/100
164/164 - 15s - loss: 1.3462 - accuracy: 0.5915 - val_loss: 1.6239 - val_accuracy: 0.5483
Epoch 36/100
164/164 - 14s - loss: 1.3322 - accuracy: 0.6022 - val_loss: 1.7032 - val_accuracy: 0.5404
Epoch 37/100
164/164 - 14s - loss: 1.3003 - accuracy: 0.6058 - val_loss: 1.6447 - val_accuracy: 0.5521
Epoch 38/100
164/164 - 14s - loss: 1.2984 - accuracy: 0.6071 - val_loss: 1.5538 - val_accuracy: 0.5555
Epoch 39/100
164/164 - 14s - loss: 1.2832 - accuracy: 0.6121 - val_loss: 1.6071 - val_accuracy: 0.5502
Epoch 40/100
164/164 - 14s - loss: 1.2540 - accuracy: 0.6140 - val_loss: 1.5911 - val_accuracy: 0.5502
Epoch 41/100
164/164 - 14s - loss: 1.2246 - accuracy: 0.6174 - val_loss: 1.6876 - val_accuracy: 0.5271
Epoch 42/100
164/164 - 14s - loss: 1.2362 - accuracy: 0.6127 - val_loss: 1.6231 - val_accuracy: 0.5574
Epoch 43/100
164/164 - 14s - loss: 1.2477 - accuracy: 0.6205 - val_loss: 1.6569 - val_accuracy: 0.5472
Epoch 44/100
164/164 - 14s - loss: 1.2106 - accuracy: 0.6296 - val_loss: 1.6912 - val_accuracy: 0.5464
Epoch 45/100
164/164 - 14s - loss: 1.1919 - accuracy: 0.6338 - val_loss: 1.7080 - val_accuracy: 0.5479
Epoch 46/100
164/164 - 14s - loss: 1.1745 - accuracy: 0.6404 - val_loss: 1.6874 - val_accuracy: 0.5570
Epoch 47/100
164/164 - 14s - loss: 1.1865 - accuracy: 0.6376 - val_loss: 1.6047 - val_accuracy: 0.5676
Epoch 48/100
164/164 - 14s - loss: 1.1542 - accuracy: 0.6426 - val_loss: 1.6842 - val_accuracy: 0.5529
Epoch 49/100
164/164 - 14s - loss: 1.1808 - accuracy: 0.6390 - val_loss: 1.6063 - val_accuracy: 0.5695
Epoch 50/100
164/164 - 14s - loss: 1.1563 - accuracy: 0.6372 - val_loss: 1.6907 - val_accuracy: 0.5688
Epoch 51/100
164/164 - 14s - loss: 1.1167 - accuracy: 0.6593 - val_loss: 1.7514 - val_accuracy: 0.5589
Epoch 52/100
164/164 - 14s - loss: 1.0921 - accuracy: 0.6590 - val_loss: 1.6941 - val_accuracy: 0.5673
Epoch 53/100
164/164 - 14s - loss: 1.0993 - accuracy: 0.6618 - val_loss: 1.6875 - val_accuracy: 0.5582
Epoch 54/100
164/164 - 14s - loss: 1.1187 - accuracy: 0.6533 - val_loss: 1.7844 - val_accuracy: 0.5339
Epoch 55/100
164/164 - 14s - loss: 1.0873 - accuracy: 0.6615 - val_loss: 1.7436 - val_accuracy: 0.5536
Epoch 56/100
164/164 - 14s - loss: 1.1057 - accuracy: 0.6677 - val_loss: 1.6872 - val_accuracy: 0.5707
Epoch 57/100
164/164 - 14s - loss: 1.0703 - accuracy: 0.6634 - val_loss: 1.7581 - val_accuracy: 0.5597
Epoch 58/100
164/164 - 14s - loss: 1.0321 - accuracy: 0.6771 - val_loss: 1.6671 - val_accuracy: 0.5646
Epoch 00058: early stopping
Test loss: 1.6731694349966162
Test accuracy: 0.5646078
               precision    recall  f1-score   support

        daisy       0.89      0.60      0.72       255
    dandelion       0.65      0.79      0.72       351
         rose       0.66      0.43      0.52       261
    sunflower       0.71      0.85      0.77       244
        tulip       0.51      0.72      0.60       328
        apple       0.44      0.24      0.31        34
       banana       0.58      0.42      0.49        33
     beetroot       0.42      0.52      0.47        33
  bell pepper       0.14      0.09      0.11        34
      cabbage       0.52      0.48      0.50        33
     capsicum       0.44      0.21      0.29        33
       carrot       0.64      0.79      0.71        34
  cauliflower       0.64      0.48      0.55        33
chilli pepper       0.37      0.21      0.27        33
         corn       0.15      0.12      0.13        34
     cucumber       0.40      0.52      0.45        33
     eggplant       0.48      0.39      0.43        33
       garlic       0.52      0.35      0.42        34
       ginger       0.55      0.55      0.55        33
       grapes       0.54      0.64      0.58        33
     jalepeno       0.43      0.26      0.33        34
         kiwi       0.60      0.73      0.66        33
        lemon       0.53      0.52      0.52        33
      lettuce       0.33      0.59      0.43        34
        mango       0.27      0.18      0.22        33
        onion       0.59      0.30      0.40        33
       orange       0.61      0.32      0.42        34
      paprika       0.44      0.21      0.29        33
         pear       0.52      0.36      0.43        33
         peas       0.48      0.35      0.41        34
    pineapple       0.55      0.55      0.55        33
  pomegranate       0.49      0.67      0.56        33
       potato       0.30      0.32      0.31        34
      raddish       0.50      0.30      0.38        33
    soy beans       0.37      0.55      0.44        33
      spinach       0.35      0.32      0.34        34
    sweetcorn       0.42      0.61      0.49        33
  sweetpotato       0.36      0.55      0.43        33
       tomato       0.42      0.44      0.43        34
       turnip       0.58      0.55      0.56        33
   watermelon       0.34      0.39      0.37        33

     accuracy                           0.56      2639
    macro avg       0.48      0.45      0.45      2639
 weighted avg       0.57      0.56      0.55      2639

[[154  39  10 ...   1   0   0]
 [  8 278   3 ...   0   0   0]
 [  4  15 113 ...   3   1   1]
 ...
 [  0   0   0 ...  15   0   2]
 [  1   0   2 ...   0  18   0]
 [  0   0   2 ...   3   0  13]]
(5278, 128, 128, 3)
(5278, 41)
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_3 (Conv2D)            (None, 124, 124, 16)      1216      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 41, 41, 16)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 37, 37, 64)        25664     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 10, 10, 256)       147712    
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 5, 5, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6400)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2560)              16386560  
_________________________________________________________________
dropout_4 (Dropout)          (None, 2560)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 640)               1639040   
_________________________________________________________________
dense_6 (Dense)              (None, 128)               82048     
_________________________________________________________________
dropout_5 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 41)                5289      
=================================================================
Total params: 18,287,529
Trainable params: 18,287,529
Non-trainable params: 0
_________________________________________________________________
(5278, 128, 128, 3)
(5278, 41)
(2639, 128, 128, 3)
(2639, 41)
Epoch 1/100
164/164 - 15s - loss: 3.1542 - accuracy: 0.1853 - val_loss: 2.7812 - val_accuracy: 0.2383
Epoch 2/100
164/164 - 15s - loss: 2.6389 - accuracy: 0.2688 - val_loss: 2.3655 - val_accuracy: 0.3236
Epoch 3/100
164/164 - 15s - loss: 2.4346 - accuracy: 0.3096 - val_loss: 2.2823 - val_accuracy: 0.3467
Epoch 4/100
164/164 - 15s - loss: 2.3147 - accuracy: 0.3414 - val_loss: 2.1186 - val_accuracy: 0.3846
Epoch 5/100
164/164 - 15s - loss: 2.1914 - accuracy: 0.3683 - val_loss: 2.1008 - val_accuracy: 0.3990
Epoch 6/100
164/164 - 15s - loss: 2.1311 - accuracy: 0.3812 - val_loss: 2.0793 - val_accuracy: 0.3960
Epoch 7/100
164/164 - 15s - loss: 2.0669 - accuracy: 0.3946 - val_loss: 1.9240 - val_accuracy: 0.4373
Epoch 8/100
164/164 - 15s - loss: 1.9918 - accuracy: 0.4182 - val_loss: 1.9535 - val_accuracy: 0.4343
Epoch 9/100
164/164 - 15s - loss: 1.9617 - accuracy: 0.4213 - val_loss: 1.8966 - val_accuracy: 0.4517
Epoch 10/100
164/164 - 15s - loss: 1.8981 - accuracy: 0.4340 - val_loss: 1.7891 - val_accuracy: 0.4775
Epoch 11/100
164/164 - 15s - loss: 1.8589 - accuracy: 0.4377 - val_loss: 1.8700 - val_accuracy: 0.4634
Epoch 12/100
164/164 - 15s - loss: 1.8066 - accuracy: 0.4661 - val_loss: 1.8801 - val_accuracy: 0.4540
Epoch 13/100
164/164 - 15s - loss: 1.7721 - accuracy: 0.4787 - val_loss: 1.7875 - val_accuracy: 0.4812
Epoch 14/100
164/164 - 15s - loss: 1.7720 - accuracy: 0.4766 - val_loss: 1.7337 - val_accuracy: 0.4937
Epoch 15/100
164/164 - 14s - loss: 1.7165 - accuracy: 0.4907 - val_loss: 1.7775 - val_accuracy: 0.4831
Epoch 16/100
164/164 - 15s - loss: 1.6827 - accuracy: 0.5059 - val_loss: 1.7325 - val_accuracy: 0.5006
Epoch 17/100
164/164 - 14s - loss: 1.6467 - accuracy: 0.5082 - val_loss: 1.7538 - val_accuracy: 0.5089
Epoch 18/100
164/164 - 15s - loss: 1.5986 - accuracy: 0.5246 - val_loss: 1.7514 - val_accuracy: 0.5051
Epoch 19/100
164/164 - 14s - loss: 1.6077 - accuracy: 0.5149 - val_loss: 1.7246 - val_accuracy: 0.5146
Epoch 20/100
164/164 - 14s - loss: 1.5936 - accuracy: 0.5225 - val_loss: 1.8002 - val_accuracy: 0.5078
Epoch 21/100
164/164 - 14s - loss: 1.5539 - accuracy: 0.5341 - val_loss: 1.6706 - val_accuracy: 0.5282
Epoch 22/100
164/164 - 14s - loss: 1.5317 - accuracy: 0.5343 - val_loss: 1.7373 - val_accuracy: 0.5150
Epoch 23/100
164/164 - 14s - loss: 1.5035 - accuracy: 0.5412 - val_loss: 1.6457 - val_accuracy: 0.5218
Epoch 24/100
164/164 - 14s - loss: 1.5058 - accuracy: 0.5437 - val_loss: 1.7026 - val_accuracy: 0.5161
Epoch 25/100
164/164 - 14s - loss: 1.4550 - accuracy: 0.5528 - val_loss: 1.7308 - val_accuracy: 0.5157
Epoch 26/100
164/164 - 14s - loss: 1.4319 - accuracy: 0.5705 - val_loss: 1.6106 - val_accuracy: 0.5415
Epoch 27/100
164/164 - 14s - loss: 1.4278 - accuracy: 0.5667 - val_loss: 1.6590 - val_accuracy: 0.5506
Epoch 28/100
164/164 - 14s - loss: 1.4233 - accuracy: 0.5669 - val_loss: 1.7225 - val_accuracy: 0.5229
Epoch 29/100
164/164 - 14s - loss: 1.3896 - accuracy: 0.5768 - val_loss: 1.6027 - val_accuracy: 0.5449
Epoch 30/100
164/164 - 14s - loss: 1.3623 - accuracy: 0.5879 - val_loss: 1.6597 - val_accuracy: 0.5309
Epoch 31/100
164/164 - 14s - loss: 1.3959 - accuracy: 0.5736 - val_loss: 1.6046 - val_accuracy: 0.5525
Epoch 32/100
164/164 - 14s - loss: 1.3223 - accuracy: 0.5896 - val_loss: 1.7407 - val_accuracy: 0.5339
Epoch 33/100
164/164 - 14s - loss: 1.2980 - accuracy: 0.5993 - val_loss: 1.6148 - val_accuracy: 0.5487
Epoch 34/100
164/164 - 14s - loss: 1.3195 - accuracy: 0.6008 - val_loss: 1.7071 - val_accuracy: 0.5487
Epoch 35/100
164/164 - 14s - loss: 1.2937 - accuracy: 0.6058 - val_loss: 1.6117 - val_accuracy: 0.5627
Epoch 36/100
164/164 - 14s - loss: 1.2838 - accuracy: 0.6062 - val_loss: 1.7818 - val_accuracy: 0.5275
Epoch 37/100
164/164 - 14s - loss: 1.2459 - accuracy: 0.6166 - val_loss: 1.7644 - val_accuracy: 0.5392
Epoch 38/100
164/164 - 14s - loss: 1.2463 - accuracy: 0.6102 - val_loss: 1.5963 - val_accuracy: 0.5616
Epoch 39/100
164/164 - 14s - loss: 1.2307 - accuracy: 0.6250 - val_loss: 1.6557 - val_accuracy: 0.5589
Epoch 40/100
164/164 - 14s - loss: 1.1796 - accuracy: 0.6307 - val_loss: 1.5767 - val_accuracy: 0.5801
Epoch 41/100
164/164 - 14s - loss: 1.2256 - accuracy: 0.6165 - val_loss: 1.6949 - val_accuracy: 0.5782
Epoch 42/100
164/164 - 14s - loss: 1.1680 - accuracy: 0.6346 - val_loss: 1.6100 - val_accuracy: 0.5669
Epoch 43/100
164/164 - 14s - loss: 1.1791 - accuracy: 0.6353 - val_loss: 1.6596 - val_accuracy: 0.5635
Epoch 44/100
164/164 - 14s - loss: 1.1627 - accuracy: 0.6424 - val_loss: 1.6503 - val_accuracy: 0.5597
Epoch 45/100
164/164 - 14s - loss: 1.1799 - accuracy: 0.6310 - val_loss: 1.6187 - val_accuracy: 0.5733
Epoch 46/100
164/164 - 14s - loss: 1.1414 - accuracy: 0.6409 - val_loss: 1.6520 - val_accuracy: 0.5582
Epoch 47/100
164/164 - 14s - loss: 1.0781 - accuracy: 0.6677 - val_loss: 1.6163 - val_accuracy: 0.5836
Epoch 48/100
164/164 - 14s - loss: 1.1292 - accuracy: 0.6514 - val_loss: 1.6475 - val_accuracy: 0.5775
Epoch 49/100
164/164 - 14s - loss: 1.0975 - accuracy: 0.6611 - val_loss: 1.6891 - val_accuracy: 0.5604
Epoch 50/100
164/164 - 15s - loss: 1.0936 - accuracy: 0.6677 - val_loss: 1.6251 - val_accuracy: 0.5820
Epoch 51/100
164/164 - 14s - loss: 1.0712 - accuracy: 0.6599 - val_loss: 1.6676 - val_accuracy: 0.5657
Epoch 52/100
164/164 - 15s - loss: 1.0461 - accuracy: 0.6719 - val_loss: 1.7219 - val_accuracy: 0.5801
Epoch 53/100
164/164 - 14s - loss: 1.0841 - accuracy: 0.6729 - val_loss: 1.6678 - val_accuracy: 0.5741
Epoch 54/100
164/164 - 15s - loss: 1.0774 - accuracy: 0.6655 - val_loss: 1.6669 - val_accuracy: 0.5858
Epoch 55/100
164/164 - 14s - loss: 1.0137 - accuracy: 0.6773 - val_loss: 1.7082 - val_accuracy: 0.5987
Epoch 56/100
164/164 - 14s - loss: 1.0183 - accuracy: 0.6843 - val_loss: 1.7347 - val_accuracy: 0.5718
Epoch 57/100
164/164 - 14s - loss: 1.0311 - accuracy: 0.6782 - val_loss: 1.8077 - val_accuracy: 0.5445
Epoch 58/100
164/164 - 14s - loss: 1.0053 - accuracy: 0.6847 - val_loss: 1.8339 - val_accuracy: 0.5676
Epoch 59/100
164/164 - 14s - loss: 1.0224 - accuracy: 0.6783 - val_loss: 1.8882 - val_accuracy: 0.5612
Epoch 60/100
164/164 - 14s - loss: 0.9972 - accuracy: 0.6899 - val_loss: 1.7550 - val_accuracy: 0.5771
Epoch 00060: early stopping
Test loss: 1.8021697657448905
Test accuracy: 0.57711256
               precision    recall  f1-score   support

        daisy       0.80      0.75      0.78       255
    dandelion       0.79      0.76      0.78       350
         rose       0.51      0.61      0.56       262
    sunflower       0.74      0.85      0.79       244
        tulip       0.56      0.60      0.58       328
        apple       0.53      0.30      0.38        33
       banana       0.65      0.38      0.48        34
     beetroot       0.41      0.42      0.42        33
  bell pepper       0.33      0.36      0.35        33
      cabbage       0.88      0.41      0.56        34
     capsicum       0.20      0.09      0.13        33
       carrot       0.74      0.70      0.72        33
  cauliflower       0.45      0.53      0.49        34
chilli pepper       0.35      0.33      0.34        33
         corn       0.27      0.12      0.17        33
     cucumber       0.56      0.29      0.38        34
     eggplant       0.47      0.21      0.29        33
       garlic       0.50      0.45      0.48        33
       ginger       0.53      0.50      0.52        34
       grapes       0.53      0.52      0.52        33
     jalepeno       0.44      0.21      0.29        33
         kiwi       0.64      0.53      0.58        34
        lemon       0.41      0.27      0.33        33
      lettuce       0.42      0.42      0.42        33
        mango       0.53      0.24      0.33        34
        onion       0.48      0.48      0.48        33
       orange       0.39      0.48      0.43        33
      paprika       0.28      0.32      0.30        34
         pear       0.53      0.52      0.52        33
         peas       0.31      0.70      0.43        33
    pineapple       0.67      0.35      0.46        34
  pomegranate       0.31      0.45      0.37        33
       potato       0.43      0.36      0.39        33
      raddish       0.68      0.38      0.49        34
    soy beans       0.36      0.70      0.47        33
      spinach       0.23      0.33      0.28        33
    sweetcorn       0.48      0.74      0.58        34
  sweetpotato       0.35      0.21      0.26        33
       tomato       0.57      0.61      0.59        33
       turnip       0.53      0.68      0.60        34
   watermelon       0.50      0.30      0.38        33

     accuracy                           0.58      2639
    macro avg       0.50      0.45      0.46      2639
 weighted avg       0.59      0.58      0.57      2639

[[192  17  15 ...   0   1   0]
 [ 16 267  13 ...   0   0   0]
 [  7   9 161 ...   3   3   1]
 ...
 [  0   0   1 ...  20   0   0]
 [  0   1   3 ...   0  23   0]
 [  1   0   0 ...   1   0  10]]
(5278, 128, 128, 3)
(5278, 41)
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 124, 124, 16)      1216      
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 41, 41, 16)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 37, 37, 64)        25664     
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 10, 10, 256)       147712    
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 5, 5, 256)         0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 5, 5, 256)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 6400)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 2560)              16386560  
_________________________________________________________________
dropout_7 (Dropout)          (None, 2560)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 640)               1639040   
_________________________________________________________________
dense_10 (Dense)             (None, 128)               82048     
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_11 (Dense)             (None, 41)                5289      
=================================================================
Total params: 18,287,529
Trainable params: 18,287,529
Non-trainable params: 0
_________________________________________________________________
(5278, 128, 128, 3)
(5278, 41)
(2639, 128, 128, 3)
(2639, 41)
Epoch 1/100
164/164 - 15s - loss: 3.1286 - accuracy: 0.2030 - val_loss: 2.7006 - val_accuracy: 0.2300
Epoch 2/100
164/164 - 15s - loss: 2.6083 - accuracy: 0.2825 - val_loss: 2.6340 - val_accuracy: 0.2842
Epoch 3/100
164/164 - 15s - loss: 2.4132 - accuracy: 0.3254 - val_loss: 2.2087 - val_accuracy: 0.3577
Epoch 4/100
164/164 - 15s - loss: 2.2817 - accuracy: 0.3502 - val_loss: 2.1590 - val_accuracy: 0.3763
Epoch 5/100
164/164 - 15s - loss: 2.1781 - accuracy: 0.3751 - val_loss: 2.1308 - val_accuracy: 0.3759
Epoch 6/100
164/164 - 15s - loss: 2.0909 - accuracy: 0.3917 - val_loss: 2.2159 - val_accuracy: 0.3812
Epoch 7/100
164/164 - 15s - loss: 2.0316 - accuracy: 0.4074 - val_loss: 2.0631 - val_accuracy: 0.3964
Epoch 8/100
164/164 - 15s - loss: 1.9832 - accuracy: 0.4192 - val_loss: 2.0366 - val_accuracy: 0.4058
Epoch 9/100
164/164 - 15s - loss: 1.9387 - accuracy: 0.4392 - val_loss: 1.9302 - val_accuracy: 0.4324
Epoch 10/100
164/164 - 15s - loss: 1.8777 - accuracy: 0.4510 - val_loss: 1.9245 - val_accuracy: 0.4452
Epoch 11/100
164/164 - 15s - loss: 1.8385 - accuracy: 0.4634 - val_loss: 2.0305 - val_accuracy: 0.4221
Epoch 12/100
164/164 - 15s - loss: 1.8032 - accuracy: 0.4731 - val_loss: 1.8374 - val_accuracy: 0.4676
Epoch 13/100
164/164 - 15s - loss: 1.7657 - accuracy: 0.4821 - val_loss: 1.8323 - val_accuracy: 0.4718
Epoch 14/100
164/164 - 15s - loss: 1.7525 - accuracy: 0.4783 - val_loss: 1.8662 - val_accuracy: 0.4695
Epoch 15/100
164/164 - 15s - loss: 1.6961 - accuracy: 0.4954 - val_loss: 1.8064 - val_accuracy: 0.4816
Epoch 16/100
164/164 - 15s - loss: 1.6746 - accuracy: 0.4956 - val_loss: 1.8568 - val_accuracy: 0.4737
Epoch 17/100
164/164 - 15s - loss: 1.6715 - accuracy: 0.5034 - val_loss: 1.9050 - val_accuracy: 0.4790
Epoch 18/100
164/164 - 14s - loss: 1.6352 - accuracy: 0.5051 - val_loss: 1.7574 - val_accuracy: 0.5078
Epoch 19/100
164/164 - 15s - loss: 1.5654 - accuracy: 0.5358 - val_loss: 1.8121 - val_accuracy: 0.5021
Epoch 20/100
164/164 - 14s - loss: 1.5823 - accuracy: 0.5343 - val_loss: 1.8342 - val_accuracy: 0.4873
Epoch 21/100
164/164 - 15s - loss: 1.5163 - accuracy: 0.5391 - val_loss: 1.7501 - val_accuracy: 0.5119
Epoch 22/100
164/164 - 14s - loss: 1.5257 - accuracy: 0.5311 - val_loss: 1.8864 - val_accuracy: 0.5025
Epoch 23/100
164/164 - 14s - loss: 1.5137 - accuracy: 0.5399 - val_loss: 1.7437 - val_accuracy: 0.5040
Epoch 24/100
164/164 - 14s - loss: 1.5002 - accuracy: 0.5457 - val_loss: 1.8046 - val_accuracy: 0.5055
Epoch 25/100
164/164 - 14s - loss: 1.4632 - accuracy: 0.5614 - val_loss: 1.7050 - val_accuracy: 0.5222
Epoch 26/100
164/164 - 14s - loss: 1.4649 - accuracy: 0.5602 - val_loss: 1.8111 - val_accuracy: 0.4835
Epoch 27/100
164/164 - 14s - loss: 1.4324 - accuracy: 0.5732 - val_loss: 1.6202 - val_accuracy: 0.5423
Epoch 28/100
164/164 - 14s - loss: 1.3984 - accuracy: 0.5722 - val_loss: 1.6578 - val_accuracy: 0.5445
Epoch 29/100
164/164 - 15s - loss: 1.3641 - accuracy: 0.5860 - val_loss: 1.6781 - val_accuracy: 0.5479
Epoch 30/100
164/164 - 14s - loss: 1.3937 - accuracy: 0.5721 - val_loss: 1.6863 - val_accuracy: 0.5207
Epoch 31/100
164/164 - 14s - loss: 1.3819 - accuracy: 0.5812 - val_loss: 1.7976 - val_accuracy: 0.5165
Epoch 32/100
164/164 - 14s - loss: 1.3341 - accuracy: 0.5986 - val_loss: 1.7567 - val_accuracy: 0.5252
Epoch 33/100
164/164 - 14s - loss: 1.3085 - accuracy: 0.5953 - val_loss: 1.7282 - val_accuracy: 0.5339
Epoch 34/100
164/164 - 14s - loss: 1.2835 - accuracy: 0.6058 - val_loss: 1.5995 - val_accuracy: 0.5548
Epoch 35/100
164/164 - 14s - loss: 1.2999 - accuracy: 0.6153 - val_loss: 1.6681 - val_accuracy: 0.5673
Epoch 36/100
164/164 - 14s - loss: 1.3229 - accuracy: 0.6093 - val_loss: 1.6193 - val_accuracy: 0.5559
Epoch 37/100
164/164 - 14s - loss: 1.2628 - accuracy: 0.6058 - val_loss: 1.6205 - val_accuracy: 0.5563
Epoch 38/100
164/164 - 14s - loss: 1.2680 - accuracy: 0.6119 - val_loss: 1.8997 - val_accuracy: 0.5131
Epoch 39/100
164/164 - 14s - loss: 1.2379 - accuracy: 0.6292 - val_loss: 1.7943 - val_accuracy: 0.5316
Epoch 40/100
164/164 - 14s - loss: 1.2347 - accuracy: 0.6179 - val_loss: 1.6307 - val_accuracy: 0.5585
Epoch 41/100
164/164 - 14s - loss: 1.1836 - accuracy: 0.6308 - val_loss: 1.6768 - val_accuracy: 0.5517
Epoch 42/100
164/164 - 14s - loss: 1.1743 - accuracy: 0.6330 - val_loss: 1.6473 - val_accuracy: 0.5578
Epoch 43/100
164/164 - 14s - loss: 1.2193 - accuracy: 0.6285 - val_loss: 1.6003 - val_accuracy: 0.5638
Epoch 44/100
164/164 - 14s - loss: 1.1496 - accuracy: 0.6420 - val_loss: 1.6739 - val_accuracy: 0.5540
Epoch 45/100
164/164 - 14s - loss: 1.1468 - accuracy: 0.6462 - val_loss: 1.7076 - val_accuracy: 0.5676
Epoch 46/100
164/164 - 14s - loss: 1.0907 - accuracy: 0.6611 - val_loss: 1.8278 - val_accuracy: 0.5445
Epoch 47/100
164/164 - 14s - loss: 1.1535 - accuracy: 0.6403 - val_loss: 1.9147 - val_accuracy: 0.5195
Epoch 48/100
164/164 - 14s - loss: 1.1021 - accuracy: 0.6618 - val_loss: 1.8511 - val_accuracy: 0.5502
Epoch 49/100
164/164 - 14s - loss: 1.1146 - accuracy: 0.6536 - val_loss: 1.6761 - val_accuracy: 0.5881
Epoch 50/100
164/164 - 14s - loss: 1.0832 - accuracy: 0.6621 - val_loss: 1.6994 - val_accuracy: 0.5650
Epoch 51/100
164/164 - 14s - loss: 1.0831 - accuracy: 0.6673 - val_loss: 1.6881 - val_accuracy: 0.5756
Epoch 52/100
164/164 - 14s - loss: 1.0602 - accuracy: 0.6681 - val_loss: 1.8064 - val_accuracy: 0.5574
Epoch 53/100
164/164 - 14s - loss: 1.0569 - accuracy: 0.6707 - val_loss: 1.7301 - val_accuracy: 0.5688
Epoch 54/100
164/164 - 14s - loss: 1.0666 - accuracy: 0.6666 - val_loss: 1.6692 - val_accuracy: 0.5779
Epoch 00054: early stopping
Test loss: 1.6658294671590963
Test accuracy: 0.5778704
               precision    recall  f1-score   support

        daisy       0.81      0.73      0.77       254
    dandelion       0.83      0.70      0.76       351
         rose       0.55      0.59      0.57       261
    sunflower       0.64      0.90      0.75       245
        tulip       0.58      0.66      0.62       328
        apple       0.47      0.27      0.35        33
       banana       0.50      0.24      0.33        33
     beetroot       0.62      0.44      0.52        34
  bell pepper       0.30      0.21      0.25        33
      cabbage       0.60      0.79      0.68        33
     capsicum       0.36      0.12      0.18        34
       carrot       0.79      0.79      0.79        33
  cauliflower       0.44      0.48      0.46        33
chilli pepper       0.22      0.24      0.23        34
         corn       0.17      0.12      0.14        33
     cucumber       0.41      0.39      0.40        33
     eggplant       0.48      0.35      0.41        34
       garlic       0.50      0.61      0.55        33
       ginger       0.47      0.42      0.44        33
       grapes       0.54      0.41      0.47        34
     jalepeno       0.29      0.39      0.33        33
         kiwi       0.73      0.67      0.70        33
        lemon       0.38      0.29      0.33        34
      lettuce       0.44      0.24      0.31        33
        mango       0.29      0.12      0.17        33
        onion       0.65      0.50      0.57        34
       orange       0.30      0.42      0.35        33
      paprika       0.52      0.48      0.50        33
         pear       0.67      0.41      0.51        34
         peas       0.62      0.30      0.41        33
    pineapple       0.36      0.58      0.44        33
  pomegranate       0.63      0.65      0.64        34
       potato       0.29      0.42      0.34        33
      raddish       0.33      0.18      0.24        33
    soy beans       0.62      0.59      0.61        34
      spinach       0.35      0.73      0.48        33
    sweetcorn       0.46      0.58      0.51        33
  sweetpotato       0.35      0.44      0.39        34
       tomato       0.38      0.55      0.45        33
       turnip       0.69      0.33      0.45        33
   watermelon       0.48      0.32      0.39        34

     accuracy                           0.58      2639
    macro avg       0.49      0.46      0.46      2639
 weighted avg       0.59      0.58      0.57      2639

[[186  14   9 ...   0   0   1]
 [ 21 244   9 ...   0   0   0]
 [ 11   5 154 ...   0   0   1]
 ...
 [  0   0   0 ...  18   0   2]
 [  0   1   3 ...   0  11   0]
 [  0   0   4 ...   6   0  11]]
[0.5646078, 0.57711256, 0.5778704]
0.5731969
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 3588085584819727303
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 5083824128
locality {
  bus_id: 1
  links {
  }
}
incarnation: 9144830149461283795
physical_device_desc: "device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1"
]
